---
title: "Homework 5"
author: "Noah Kawasaki"
date: "6/6/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo=TRUE,
  message=FALSE, 
  warning=FALSE,
  fig.height=3,
  root.dir='/Users/noahkawasaki/Desktop/ECON 144/Textbook_data'
)

library(tidyverse)
library(tseries)
library(forecast)
library(readxl)
library(TSA)
library(rugarch)
library(fGarch)
library(ggfortify)
library(quantmod)

setwd('/Users/noahkawasaki/Desktop/ECON 144/Textbook_data')
```

## Textbook A

## Problem 14.2
```{r}
m1 <- garch.sim(alpha=c(3, 0.35), beta=c(0.6), n=500)  # 0.75
mean1 <- mean(m1)
sd1 <- sd(m1)
sm1 <- (m1-mean1)/sd1
sds1 <- sqrt((m1-mean1)^2)

m2 <- garch.sim(alpha=c(3, 0.1), beta=c(0.6), n=500)  # 0.16
mean2 <- mean(m2)
sd2 <- sd(m2)
sm2 <- (m2-mean2)/sd2
sds2 <- sqrt((m2-mean2)^2)

# Time Series
ggplot(data.frame(m1), aes(x=seq(from=1, to=length(m1)), y=m1)) +
  geom_line(color="red") +
  ylim(-18, 18) +
  ggtitle("GARCH(1,1)", "High Persistence") +
  xlab("Index") +
  ylab("Value")

ggplot(data.frame(m2), aes(x=seq(from=1, to=length(m2)), y=m2)) +
  geom_line(color="blue") +
  ylim(-18, 18) +
  ggtitle("GARCH(1,1)", "Low Persistence") +
  xlab("Index") +
  ylab("Value")

# Conditional Standard Deviations
ggplot(data.frame(m1), aes(x=seq(from=1, to=length(m1)), y=sds1)) +
  geom_line(color="red") +
  ylim(0,20) +
  ggtitle("GARCH(1,1) Conditional Standard Deviation", "High Persistence") +
  xlab("Index") +
  ylab("Value")

ggplot(data.frame(m1), aes(x=seq(from=1, to=length(m1)), y=sds2)) +
  geom_line(color="blue") +
  ylim(0,20) +
  ggtitle("GARCH(1,1) Conditional Standard Deviation", "Low Persistence") +
  xlab("Index") +
  ylab("Value")
```

From the simulated time series plots we have a high persistence series with a persistence parameter of 0.875 and a low persistence series with a persistence parameter of 0.25. It is clear that the high persistence series has more volatility clustering and overall volatility, while the lowe persistence series resembles as MA process.


```{r}
# Histogram of original time series
ggplot(data.frame(m1), aes(x=m1)) +
  geom_histogram(fill="red", color="black") +
  xlim(-21, 21) +
  ggtitle("Histogram GARCH(1,1)", "High Persistence") +
  xlab("Value") +
  ylab("Count") 

ggplot(data.frame(m2), aes(x=m2)) +
  geom_histogram(fill="blue", color="black") +
  xlim(-21, 21) +
  ggtitle("Histogram GARCH(1,1)", "Low Persistence") +
  xlab("Value") +
  ylab("Count")


# Histogram of standardized time series
ggplot(data.frame(sm1), aes(x=sm1)) +
  geom_histogram(fill="red", color="black") +
  xlim(-4, 4) +
  ggtitle("Histogram of Standardized GARCH(1,1)", "High Persistence") +
  xlab("Value") +
  ylab("Count") 

ggplot(data.frame(sm2), aes(x=sm2)) +
  geom_histogram(fill="blue", color="black") +
  xlim(-4, 4) +
  ggtitle("Histogram of Standardized GARCH(1,1)", "Low Persistence") +
  xlab("Value") +
  ylab("Count")
```

The histogram of the high persistence series shows fatter tails than the low persistence series. This is expected behavior as the persistence causes high and low extreme values to persist over time and thus have higher frequencies. The standardized series histograms exhibit the same patterns but less extremely (because they are standardized).


```{r}
# ACF/PACF original time series
par(mfrow=c(1,2))
acf(m1, main="ACF - GARCH(1,1) High Persistence")
pacf(m1, main="PACF - GARCH(1,1) High Persistence")

par(mfrow=c(1,2))
acf(m2, main="ACF - GARCH(1,1) Low Persistence")
pacf(m2, main="PACF - GARCH(1,1) Low Persistence")


# ACF/PACF standardized series
par(mfrow=c(1,2))
acf(sm1, main="ACF - GARCH(1,1) Standardized, High Persistence")
pacf(sm1, main="PACF - GARCH(1,1) Standardized, High Persistence")

par(mfrow=c(1,2))
acf(sm2, main="ACF - GARCH(1,1) Standardized, Low Persistence")
pacf(sm2, main="PACF - GARCH(1,1) Standardized, Low Persistence")
```

The ACF and PACF's of the original series and standardized original series dont really show any time dependence besides up to a lag of 1 in the high persistence series. 


```{r}
# ACF/PACF squared time series
par(mfrow=c(1,2))
acf(m1^2, main="ACF - GARCH(1,1) Squared, High Persistence")
pacf(m1^2, main="PACF - GARCH(1,1) Squared, High Persistence")

par(mfrow=c(1,2))
acf(m2^2, main="ACF - GARCH(1,1) Squared, Low Persistence")
pacf(m2^2, main="PACF - GARCH(1,1) Squared, Low Persistence")

# ACF/PACF squared standardized series
par(mfrow=c(1,2))
acf(sm1^2, main="ACF - GARCH(1,1) Standardized Squared, High Persistence")
pacf(sm1^2, main="PACF - GARCH(1,1) Standardized Squared, High Persistence")

par(mfrow=c(1,2))
acf(sm2^2, main="ACF - GARCH(1,1) Standardized Squared, Low Persistence")
pacf(sm2^2, main="PACF - GARCH(1,1) Standardized Squared, Low Persistence")
```

Now, the squared series shows much more time dependence. This is because squaring the series essentially shows us the magnitude of variation from the expected value. Now we see the high persistence series showing some strong time dependence with a slow decay to zero. The low persistence series show only dependence up to one lag.


## Problem 14.3
```{r}
sp <- read_xls("/Users/noahkawasaki/Desktop/ECON 144/Textbook_data/Chapter14_exercises_data.xls")
df <- data.frame(sp$Date, sp$`Adj Close`) %>%
  set_names("date", "adjusted") %>%
  mutate(
    return = log(adjusted) - log(lag(adjusted))
  )
df <- na.omit(df)


# Plot time series and return series
ggplot(df, aes(x=date, y=adjusted)) +
  geom_line(color="red") +
  ggtitle("S&P500 Price", "2000-2013") +
  xlab("Date") +
  ylab("Price")

ggplot(df, aes(x=date, y=return)) +
  geom_line(color="red", na.rm=TRUE) +
  ggtitle("S&P500 Returns", "2000-2013") +
  xlab("Date") +
  ylab("Rate")
```

From the time series and returns plot of the S&P 500 it is obvious that this data has periods of high volatility and low volatility. Notably, from 2004 to 2007 is a period of low variation, but from 2007 to 2010 there is some extreme deviation behavior. 


```{r}
# ACF and PACFs
par(mfrow=c(1,2))
acf(df$return, main="ACF - S&P500 Returns")
pacf(df$return, main="PACF - S&P500 Returns")

par(mfrow=c(1,2))
acf((df$return)^2, main="ACF - S&P500 Squared Returns")
pacf((df$return)^2, main="PACF - S&P500 Squared Returns")
```

If we look at the ACF and PACF's of the original returns and the squared returns we'll see that there is some variance time dependence, and that ARCH and GARCH models should be applied to tease out these signals.


```{r}
# Find best ARCH Model
arch = garch(df$return, order=c(0,11), trace=F)
summary(arch)
logLik(arch)
```

The best ARCH model for this data series is a high order one at ARCH(11). All coefficients but the a1 are statistically significant, and the model achieves a log likelihood of 10453. 


```{r}
par(mfrow=c(1,2))
acf(arch$residuals[12:length(arch$residuals)], main="ACF - ARCH(11) Residuals")
pacf(arch$residuals[12:length(arch$residuals)], main="PACF - ARCH(11) Residuals")
```

Looking at the ACF and PACF of the residuals, it appears that for the most part these signals have been accounted for. 


```{r}
# GARCH Model
garch <- garchFit(~garch(2,2), data=df$return, trace=FALSE)
summary(garch)
```

While the ARCH(11) did a good job at modeling the S&P returns, it contains 12 parameters to estimate. So we can also fit a GARCH model to achieve similar (better?) results with less computing. The chosen model is GARCH(2,2). Its coefficients are statistically significant besides the alpha1, and gets a higher log likelihood of 10487. 


```{r}
par(mfrow=c(1,2))
acf(garch@residuals, main="ACF - GARCH(2,2) Residuals")
pacf(garch@residuals, main="ACF - GARCH(2,2) Residuals")
```

The ACF and PACF of the GARCH(2,2) residuals also suggest that the model effectively captured the volatile behavior.


## Problem 14.4
```{r, fig.height=5}
# One and two step ahead forecasts
predicts <- predict(garch, n.ahead=2, plot=TRUE)
predicts
```


## Textbook B

## Problem 14.1
```{r}
## a
getSymbols("^NYA", from="1988-01-01", to="2001-12-31")
nyse <- data.frame(log(NYA$NYA.Adjusted) - log(lag(NYA$NYA.Adjusted))[-1])
nyse <- tibble::rownames_to_column(nyse) %>%
  set_names("date", "return")

train <- nyse[1:3461, ]
test <- nyse[3462:nrow(nyse), ]

ts <- ts(train$return)
ts_test <- ts(test$return)

# Plot
ggplot(train, aes(x=seq(from=1, to=length(return)), y=return, group=1)) +
  geom_line(color="green") +
  ggtitle("NYSE Daily Returns", "1988-2001") +
  xlab("Index") +
  ylab("Rate")

# AR
m = auto.arima(ts)
summary(m)

resids <- m$residuals[13:length(m$residuals)]

par(mfrow=c(1,2))
acf(resids)
pacf(resids)

par(mfrow=c(1,2))
acf((resids)^2)
pacf((resids)^2)

# GARCH
garch <- garchFit(~garch(1,2), data=resids, trace=FALSE)
summary(garch)

par(mfrow=c(1,2))
acf((garch@residuals)^2)
pacf((garch@residuals)^2)

```


## Problem 14.4
### a)
```{r, fig.height=5}
ar5 <- arma(ts, order=c(5,0))
plot(ar5$fitted.values)
```


### b)
```{r, fig.height=5}
smooth <- ets(ts, model="ANN", alpha=0.10)
plot(smooth)
```

### c)
```{r, fig.height=5}
garch11 <- garchFit(~garch(1,2), data=ts, trace=FALSE)
plot(garch11, which=1)
```


### d)
For the most part, visually these models all look very similar. However the ARCH and GARCH model's are based on actual time dependence as opposed to the mathematical strategies used in exponential smoothing. This means that the GARCH's smoothing parameter is chosen through likelihood optimization, whereas we just chose an arbitrary value for the ets model.


## Problem 14.5
### a)
```{r}
spec <- garchSpec(model=list(mu=76, alpha=0.6, beta=0, omega=3))
sim <- garchSim(spec, n=100)

fit <- garchFit(~garch(1, 1), sim, trace=FALSE)
predict(fit, n.ahead=10)
```


### b)


### c)
Answer is not necessarily. Because a GARCH model is based on the time dependence of volatility and is short term, in order to predict a next spell of bad weather that bad weather would have already needed to start. 








